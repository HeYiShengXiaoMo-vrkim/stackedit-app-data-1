```python
#  代码实战解析解  
#  numpy是用来做数据计算的  
#  matplotlib是用来做数据可视化的  
import numpy as np  
import matplotlib.pyplot as plt  
  
#  有监督的机器学习  
#  生成100个随机数，范围在0-1之间  
x = np.random.rand(100, 1)  
# 这里是要模拟出来的数据y是代表真实的数据，所以也就是y_hat + error  
#  生成100个随机数，范围在0-1之间，加上3x+2,相当于预期解加error  
y = 3 * x + 2 + np.random.rand(100, 1)  
#  在x前面加上一列全为1的列，用于计算截距  
x_b = np.c_[np.ones((100, 1)), x]  
# 实现解析解的公式来求解  .T实现矩阵转置  
theta = np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)  
print(theta) # 打印解析解  
  
#  使用模型去做预测  
x_new = np.array([[0],[2]])  # 添加两行一列的矩阵  
x_new_b = np.c_[np.ones((2, 1)), x_new]  #  添加一列全为1的列，用于计算截距  
y_predict = x_new_b.dot(theta)  #  模型预测,这里就是用解析解的公式来计算，x_new_b.dot(theta) = x_new_b * theta  
print(y_predict) # 打印预测结果  
#  画图进行展示真实的数据点和我们预测用的数据点  
plt.plot(x_new, y_predict, "r-"), plt.plot(x, y, "b.") #  画图，x_new是横坐标，y_predict是纵坐标，"r-"代表红色实线，"b."代表蓝色点  
plt.axis([0,2,0,7]) #  设置x轴和y轴的范围  
plt.show()
```


## 矩阵部分讲解
理解矩阵乘法对于这段代码来说确实很重要。让我解释一下代码中涉及的矩阵运算：

1. `x_b.T.dot(x_b)`：
   - `x_b.T` 是 x_b 的转置
   - `dot()` 方法执行矩阵乘法
   - 这一步计算 X^T * X，其中 X 是包含截距项的特征矩阵

2. `np.linalg.inv(x_b.T.dot(x_b))`：
   - `np.linalg.inv()` 计算矩阵的逆
   - 这一步计算 `(X^T * X)^(-1)`

3. `x_b.T.dot(y)`：
   - 计算 `X^T * y`，其中 y 是目标变量向量

4. `np.linalg.inv(x_b.T.dot(x_b)).dot(x_b.T).dot(y)`：
   - 这整个表达式实现了线性回归的解析解公式：`θ = (X^T * X)^(-1) * X^T * y`
   - 结果是包含截距和斜率的参数向量 θ

5. `x_new_b.dot(theta)`：
   - 使用计算出的 θ 进行预测
   - 相当于计算 y = θ_0 + θ_1 * x，其中 θ_0 是截距，θ_1 是斜率

这些矩阵运算高效地实现了线性回归的计算过程。使用矩阵运算而不是循环可以大大提高代码的效率，尤其是在处理大型数据集时。


## 使用模型做预测部分讲解

让我详细解释一下使用模型进行预测的那几行代码：

```python
x_new = np.array([[0],[2]])
x_new_b = np.c_[np.ones((2, 1)), x_new]
y_predict = x_new_b.dot(theta)
```

1. `x_new = np.array([[0],[2]])`
   - 这行创建了一个新的输入数据数组。
   - 它包含两个数据点：0 和 2。
   - 形状是 (2,1)，即两行一列。
   - 这些是我们想要预测的新 x 值。

2. `x_new_b = np.c_[np.ones((2, 1)), x_new]`
   - `np.ones((2, 1))` 创建一个 2x1 的全是 1 的数组。
   - `np.c_[]` 将这个全 1 数组与 x_new 在列方向上连接。
   - 结果是 [[1, 0], [1, 2]]。
   - 这步是为了在新数据中加入截距项，与训练数据的处理方式一致。

3. `y_predict = x_new_b.dot(theta)`
   - 这行实际上进行了预测。
   - `theta` 是之前计算出的模型参数（包含截距和斜率）。
   - `x_new_b.dot(theta)` 执行矩阵乘法，相当于应用线性回归公式：y = θ₀ + θ₁x。
   - 对于每个新的 x 值，计算对应的 y 预测值。

这几行代码展示了如何使用训练好的线性回归模型对新数据进行预测。它首先准备新的输入数据（包括添加截距项），然后应用模型参数来计算预测值。这个过程体现了机器学习模型的一个关键特性：一旦训练完成，就可以用来对新的、未见过的数据进行预测。


## 两列为1的矩阵目的

你提出了一个很好的问题。让我解释一下为什么在代码中两次添加了全为1的列：

1. 第一次添加1列（训练数据）：
```python
x_b = np.c_[np.ones((100, 1)), x]
```

2. 第二次添加1列（预测数据）：
```python
x_new_b = np.c_[np.ones((2, 1)), x_new]
```

这两次添加1列的目的是相同的，都是为了在线性回归模型中包含截距项。让我详细解释一下：

1. 线性回归的一般形式：
   - 没有截距项的线性回归：y = mx
   - 有截距项的线性回归：y = mx + b

2. 矩阵形式表示：
   - 我们用矩阵乘法来表示线性回归：y = Xθ
   - 其中 X 是特征矩阵，θ 是参数向量

3. 添加截距的作用：
   - 添加一列1相当于为模型增加一个总是为1的特征
   - 这允许模型学习一个常数项（截距），使得拟合的直线不必通过原点

4. 在训练数据中添加（x_b）：
   - 这使得模型可以学习截距
   - 结果矩阵 x_b 的形状是 (100, 2)，第一列全是1，第二列是原始的 x 值

5. 在预测数据中添加（x_new_b）：
   - 这是为了保持一致性，使预测时的数据格式与训练时相同
   - 确保我们用同样的方式（包括截距项）来应用学习到的模型

6. 数学意义：
   - 如果 θ = [θ₀, θ₁]，那么模型变成：y = θ₀ * 1 + θ₁ * x
   - 这正是我们想要的带截距的线性方程：y = mx + b，其中 θ₀ 就是 b，θ₁ 就是 m

通过这种方式，我们不需要单独处理截距，而是将其作为特征矩阵的一部分，使得整个过程更加统一和简洁。这种方法在机器学习中被广泛使用，因为它简化了模型的实现和理解。
<!--stackedit_data:
eyJoaXN0b3J5IjpbODcyMDcwNDg5LDE5MzI2NTkyMDksMTg4Mj
cyNTc1OV19
-->