# 分类算法详解

## 1. Scikit-learn基础组件

### 1.1 转换器(Transformer)
- 主要用于特征工程
- 核心方法：
  - `fit()`: 计算数据的参数（如均值、标准差）
  - `transform()`: 根据参数转换数据
  - `fit_transform()`: 组合方法，先拟合再转换
- 使用场景：标准化、特征提取等

### 1.2 估计器(Estimator)
- 机器学习算法的基本实现
- 核心流程：
  1. 实例化估计器
  2. 训练模型：`fit(X_train, y_train)`
  3. 模型评估：
     - 预测：`predict(X_test)`
     - 计算准确率：`score(X_test, y_test)`

## 2. K-近邻(KNN)算法

### 2.1 基本原理
- 核心思想：通过邻近样本的类别来预测目标样本类别
- 关键参数：K值（近邻数量）
- 距离计算方法：
  - 欧氏距离：直线距离
  - 曼哈顿距离：坐标轴距离
  - 明可夫斯基距离：欧氏距离和曼哈顿距离的一般化形式

### 2.2 K值选择
- K值过小：
  - 容易受异常点影响
  - 模型复杂度高，容易过拟合
- K值过大：
  - 容易受样本不均衡影响
  - 模型过于简单，可能欠拟合

### 2.3 算法特点
优点：
- 简单直观
- 无需训练过程
- 适用于多分类问题

缺点：
- 计算量大
- 内存消耗高
- K值选择困难
- 对数据规模敏感

使用场景：
- 小型数据集（几千到几万样本）
- 特征空间简单的分类问题

## 3. 模型选择与调优

### 3.1 交叉验证
- 用于评估模型性能
- 避免过拟合
- 提供稳定的模型评估结果

### 3.2 网格搜索
- 用于超参数优化
- 实现方式：遍历所有可能的参数组合
- 常用参数范围：
  - KNN的K值：[1, 3, 5, 7, 9, 11]等

## 4. 朴素贝叶斯算法

### 4.1 基本概念
- 基于贝叶斯定理
- "朴素"假设：特征间相互独立
- 概率相关定义：
  - 联合概率：P(A,B) 多个事件同时发生的概率
  - 条件概率：P(A|B) 在B发生的条件下A发生的概率
  - 相互独立：P(A,B) = P(A)P(B)

### 4.2 算法特点
优点：
- 对缺失数据不敏感
- 计算简单高效
- 分类准确度较高
- 特别适合文本分类

缺点：
- 特征独立性假设往往不成立
- 对特征关联性强的数据效果不佳

### 4.3 应用场景
- 文本分类
- 垃圾邮件过滤
- 情感分析

## 5. 决策树

### 5.1 基本原理
- 树形结构进行决策
- 节点表示特征
- 分支表示决策规则
- 叶节点表示分类结果

### 5.2 划分依据
- 信息论基础：
  - 信息：消除随机不确定性的内容
  - 信息熵：衡量信息量的指标
  - 信息增益：g(D,A) = H(D) - H(D|A)
    - H(D)：数据集D的信息熵
    - H(D|A)：特征A条件下D的条件熵

### 5.3 算法特点
优点：
- 可视化效果好
- 可解释性强
- 特征选择过程直观

缺点：
- 容易过拟合
- 对连续变量的处理能力较弱

## 6. 随机森林

### 6.1 基本原理
- 集成学习方法
- 多个决策树的组合
- 随机性体现：
  1. 训练集随机：Bootstrap抽样
  2. 特征随机：随机选择特征子集

### 6.2 特点
优点：
- 适合处理高维特征
- 可处理大规模数据
- 减少过拟合风险
- 预测准确度高

使用场景：
- 大规模数据集
- 高维特征数据
- 需要较高准确度的分类问题

### 6.3 重要概念
- Bootstrap抽样：有放回随机抽样
- 特征随机选择：从M个特征中随机选择m个(m<<M)
- 投票机制：多个决策树投票决定最终分类结果
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyOTk5NjExNjddfQ==
-->